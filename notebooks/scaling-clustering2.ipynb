{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sklearn.cluster\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM\n",
    "\n",
    "from fast_pytorch_kmeans import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseProjectionOperator:\n",
    "    \"\"\"\n",
    "    Note: I think the sparsity is off by a factor of two here.\n",
    "    \"\"\"\n",
    "    def __init__(self, original_dim, projection_dim, sparsity, seed=0, device='cpu'):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed) if 'cuda' in str(device) else None\n",
    "        self.device = torch.device(device)\n",
    "        self.original_dim = original_dim\n",
    "        self.lambda_ = original_dim * (1 - sparsity)\n",
    "        num_entries = torch.poisson(self.lambda_ * torch.ones(projection_dim, device=device)).int()\n",
    "        max_entries = num_entries.max()\n",
    "        self.positives = torch.randint(0, original_dim, (projection_dim, max_entries), device=device)\n",
    "        self.negatives = torch.randint(0, original_dim, (projection_dim, max_entries), device=device)\n",
    "        masks = torch.arange(max_entries, device=device).expand(projection_dim, max_entries) < num_entries.unsqueeze(-1)\n",
    "        self.positives = self.positives * masks\n",
    "        self.negatives = self.negatives * masks\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        assert x.device == self.device, \"device mismatch between projection and input\"\n",
    "        assert x.shape[-1] == self.original_dim, \"input dimension mismatch\"\n",
    "        y = x[self.positives].sum(-1) - x[self.negatives].sum(-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pythia-160m\"\n",
    "step = 143000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cache_dir = \"/om/user/ericjm/quanta-discovery/cache/\"\n",
    "pile_canonical = \"/om/user/ericjm/the_pile/the_pile_test_canonical_200k\"\n",
    "loss_threshold = 0.5 # 0.5-bit threshold\n",
    "skip = 10\n",
    "num_tokens = 100000\n",
    "proj_dim = 5000\n",
    "sparsity = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/user/ericjm/miniconda3/envs/phase-changes/lib/python3.8/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"EleutherAI/{model_name}\",\n",
    "    revision=f\"step{step}\",\n",
    "    cache_dir=os.path.join(cache_dir, model_name, f\"step{step}\"),\n",
    ")\n",
    "\n",
    "# ----- load the_pile test set -----\n",
    "dataset = datasets.load_from_disk(pile_canonical)\n",
    "\n",
    "def tokenize_sample(sample):\n",
    "    tokens = tokenizer(sample[\"text\"], return_tensors='pt', \n",
    "                        max_length=1024, truncation=True)[\"input_ids\"]\n",
    "    return {\"input_ids\": tokens}\n",
    "\n",
    "starting_indexes = np.array([0] + list(np.cumsum(dataset[\"preds_len\"])))\n",
    "\n",
    "def loss_idx_to_dataset_idx(idx):\n",
    "    \"\"\"given an idx in range(0, 10658635), return\n",
    "    a sample index in range(0, 20000) and pred-in-sample\n",
    "    index in range(0, 1023). Note token-in-sample idx is\n",
    "    exactly pred-in-sample + 1\"\"\"\n",
    "    sample_index = np.searchsorted(starting_indexes, idx, side=\"right\") - 1\n",
    "    pred_in_sample_index = idx - starting_indexes[sample_index]\n",
    "    return int(sample_index), int(pred_in_sample_index)\n",
    "\n",
    "def get_context(idx):\n",
    "    \"\"\"given idx in range(0, 10658635), return dataset sample\n",
    "    and predicted token index within sample, in range(1, 1024).\"\"\"\n",
    "    sample_index, pred_index = loss_idx_to_dataset_idx(idx)\n",
    "    return dataset[sample_index], pred_index+1\n",
    "\n",
    "def print_context(idx):\n",
    "    \"\"\"\n",
    "    given idx in range(0, 10658635), print prompt preceding the corresponding\n",
    "    prediction, and highlight the predicted token.\n",
    "    \"\"\"\n",
    "    sample, token_idx = get_context(idx)\n",
    "    prompt = sample[\"split_by_token\"][:token_idx]\n",
    "    prompt = \"\".join(prompt)\n",
    "    token = sample[\"split_by_token\"][token_idx]\n",
    "    print(prompt + \"\\033[41m\" + token + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    f\"EleutherAI/{model_name}\",\n",
    "    revision=f\"step{step}\",\n",
    "    cache_dir=os.path.join(cache_dir, model_name, f\"step{step}\"),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_70m = torch.load(\"/om/user/ericjm/quanta-discovery/cache/pythia-70m/step143000/20000_docs_10658635_tokens_losses.pt\") / np.log(2)\n",
    "losses_160m = torch.load(\"/om/user/ericjm/quanta-discovery/cache/pythia-160m/step143000/20000_docs_10658635_tokens_losses.pt\") / np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idxs = ((losses_160m < 0.5) & (losses_70m > 2.0)).nonzero()[:, 0]\n",
    "token_idxs = token_idxs[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- make the magic happen -----\n",
    "def get_flattened_gradient(model, param_subset):\n",
    "    grads = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if name in param_subset:\n",
    "            grads.append(p.grad)\n",
    "    return torch.cat([g.flatten() for g in grads])\n",
    "param_names = [n for n, _ in model.named_parameters()]\n",
    "\n",
    "highsignal_names = [name for name in param_names if \n",
    "                        ('layernorm' not in name) and \n",
    "                        ('embed' not in name)]\n",
    "\n",
    "len_g = sum(model.state_dict()[name].numel() for name in highsignal_names)\n",
    "S = len(token_idxs)\n",
    "R = SparseProjectionOperator(len_g, proj_dim, sparsity, seed=0, device=device)\n",
    "Gs = torch.zeros((S, proj_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, idx in tqdm(list(enumerate(token_idxs))):\n",
    "#     model.zero_grad()\n",
    "#     document, l = get_context(idx)\n",
    "#     prompt = document['text']\n",
    "#     tokens = tokenizer(prompt, return_tensors='pt', max_length=1024, truncation=True).to(device)\n",
    "#     logits = model(**tokens).logits\n",
    "#     targets = tokens.input_ids\n",
    "#     ls = torch.nn.functional.cross_entropy(logits[0, :-1, :], targets[0, 1:], reduction='none')\n",
    "#     ls_l = ls[l-1]\n",
    "#     ls_l.backward()\n",
    "#     g = get_flattened_gradient(model, highsignal_names)\n",
    "#     with torch.no_grad():\n",
    "#         Gs[i] = R(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gs_cpu = Gs.to('cpu')\n",
    "# torch.save(Gs_cpu, 'Gs2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = torch.load(\"../scripts/tmp/Gs2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = Gs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10000, \n",
    "                max_iter=100,\n",
    "                mode='cosine', \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used 100 iterations (231.5984s) to cluster 100000 items into 10000 clusters\n"
     ]
    }
   ],
   "source": [
    "labels = kmeans.fit_predict(Gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [l.item() for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = defaultdict(int)\n",
    "for label in labels:\n",
    "    label_counts[label] += 1\n",
    "\n",
    "labels_sorted_by_cluster_size = sorted(label_counts.keys(), key=lambda l: label_counts[l], reverse=True)\n",
    "sizes_sorted_by_cluster_sizes = [label_counts[l] for l in labels_sorted_by_cluster_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = defaultdict(list)\n",
    "for i in range(len(labels)):\n",
    "    indices[labels[i]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd85596b9384498daafd31c631a3684f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_range = range(0, 4000)\n",
    "clusters_data = defaultdict(list)\n",
    "for i in tqdm(cluster_range):\n",
    "    l = labels_sorted_by_cluster_size[i]\n",
    "    for idx_i in indices[l]:\n",
    "        idx = token_idxs[idx_i]\n",
    "        doc, token_idx_within_doc = get_context(idx)\n",
    "        tokens = doc[\"split_by_token\"]\n",
    "        clusters_data[i].append((tokens, token_idx_within_doc))\n",
    "\n",
    "torch.save((clusters_data, labels), \"../results/kmeans-first-try-clusters-160m.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phase-changes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
