{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "# from evaluate_pile_losses import evaluate_pile_losses\n",
    "# from evaluate_pile_induction_criterias import evaluate_pile_induction_criterias\n",
    "\n",
    "# import scipy.linalg\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# import sklearn.cluster\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"/om/user/ericjm/quanta-discovery/cache/\"\n",
    "pile_canonical = \"/om/user/ericjm/the_pile/the_pile_test_canonical_200k\"\n",
    "model_name = \"pythia-70m-v0\"\n",
    "step = 143000\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- load model and tokenizer -----\n",
    "assert \"pythia\" in model_name, \"must be a Pythia model\"\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    f\"EleutherAI/{model_name}\",\n",
    "    revision=f\"step{step}\",\n",
    "    cache_dir=os.path.join(cache_dir, model_name, f\"step{step}\"),\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"EleutherAI/{model_name}\",\n",
    "    revision=f\"step{step}\",\n",
    "    cache_dir=os.path.join(cache_dir, model_name, f\"step{step}\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/user/ericjm/miniconda3/envs/phase-changes/lib/python3.8/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ----- load the_pile test set -----\n",
    "dataset = datasets.load_from_disk(pile_canonical)\n",
    "\n",
    "def tokenize_sample(sample):\n",
    "    tokens = tokenizer(sample[\"text\"], return_tensors='pt', \n",
    "                        max_length=1024, truncation=True)[\"input_ids\"]\n",
    "    return {\"input_ids\": tokens}\n",
    "\n",
    "starting_indexes = np.array([0] + list(np.cumsum(dataset[\"preds_len\"])))\n",
    "\n",
    "def loss_idx_to_dataset_idx(idx):\n",
    "    \"\"\"given an idx in range(0, 10658635), return\n",
    "    a sample index in range(0, 20000) and pred-in-sample\n",
    "    index in range(0, 1023). Note token-in-sample idx is\n",
    "    exactly pred-in-sample + 1\"\"\"\n",
    "    sample_index = np.searchsorted(starting_indexes, idx, side=\"right\") - 1\n",
    "    pred_in_sample_index = idx - starting_indexes[sample_index]\n",
    "    return int(sample_index), int(pred_in_sample_index)\n",
    "\n",
    "def get_context(idx):\n",
    "    \"\"\"given idx in range(0, 10658635), return dataset sample\n",
    "    and predicted token index within sample, in range(1, 1024).\"\"\"\n",
    "    sample_index, pred_index = loss_idx_to_dataset_idx(idx)\n",
    "    return dataset[sample_index], pred_index+1\n",
    "\n",
    "def print_context(idx):\n",
    "    \"\"\"\n",
    "    given idx in range(0, 10658635), print prompt preceding the corresponding\n",
    "    prediction, and highlight the predicted token.\n",
    "    \"\"\"\n",
    "    sample, token_idx = get_context(idx)\n",
    "    prompt = sample[\"split_by_token\"][:token_idx]\n",
    "    prompt = \"\".join(prompt)\n",
    "    token = sample[\"split_by_token\"][token_idx]\n",
    "    print(prompt + \"\\033[41m\" + token + \"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.load(f\"/om/user/ericjm/quanta-discovery/cache/{model_name}/step{step}/20000_docs_10658635_tokens_losses.pt\")\n",
    "filter = torch.load(f\"/om/user/ericjm/quanta-discovery/cache/{model_name}/step{step}/20000_docs_10658635_tokens_present_trigram_filter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's choose some tokens to compute gradients for. choose relatively low-loss tokens with value False in filter\n",
    "low_loss_nontrigram_token_idxs = ((losses < 0.2) & ~filter).nonzero().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but let's actually use the tokens from a couple of clusters -- that way, we can ensure that the similarity\n",
    "# and dissimilarity of the tokens is preserved\n",
    "token_idxs, C = torch.load(\"../results/paper-replication/pythia-70m-v0_143000_0.14426950408889636_50_10000_v1.pt\")\n",
    "_, cluster_labels = torch.load(\"../results/paper-replication/400_auto_pythia-70m-v0_143000_0.14426950408889636_50_10000_v1.pt\")\n",
    "\n",
    "label_frequencies = defaultdict(int)\n",
    "for l in cluster_labels:\n",
    "    label_frequencies[l] += 1\n",
    "\n",
    "labels_sorted_by_freq = sorted(label_frequencies.keys(), key=lambda k: label_frequencies[k], reverse=True)\n",
    "# label_permutation = [labels_sorted_by_freq.index(i) for i in labels_sorted_by_freq]\n",
    "permutation = []\n",
    "indices = defaultdict(list)\n",
    "for i, cls in enumerate(cluster_labels):\n",
    "    indices[cls].append(i)\n",
    "for cls in labels_sorted_by_freq:\n",
    "    permutation.extend(indices[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_idxs = indices[labels_sorted_by_freq[200]] + indices[labels_sorted_by_freq[201]]\n",
    "C_part = C[sim_idxs, :][:, sim_idxs]\n",
    "plt.imshow(C_part, cmap=\"CMRmap\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_token_idxs = [token_idxs[i] for i in sim_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flattened_gradient(model, param_subset):\n",
    "    grads = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if name in param_subset:\n",
    "            grads.append(p.grad)\n",
    "    return torch.cat([g.flatten() for g in grads])\n",
    "param_names = [n for n, _ in model.named_parameters()]\n",
    "\n",
    "highsignal_names = [name for name in param_names if \n",
    "                        ('layernorm' not in name) and \n",
    "                        ('embed' not in name)]\n",
    "\n",
    "len_g = sum(model.state_dict()[name].numel() for name in highsignal_names)\n",
    "S = len(subset_token_idxs)\n",
    "\n",
    "C = torch.empty((S, S), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = torch.zeros((S, len_g), device=device)\n",
    "for i, idx in enumerate(subset_token_idxs):\n",
    "    model.zero_grad()\n",
    "    document, l = get_context(idx)\n",
    "    prompt = document['text']\n",
    "    tokens = tokenizer(prompt, return_tensors='pt', max_length=1024, truncation=True).to(device)\n",
    "    logits = model(**tokens).logits\n",
    "    targets = tokens.input_ids\n",
    "    ls = torch.nn.functional.cross_entropy(logits[0, :-1, :], targets[0, 1:], reduction='none')\n",
    "    ls_l = ls[l-1]\n",
    "    ls_l.backward()\n",
    "    g = get_flattened_gradient(model, highsignal_names)\n",
    "    Gs[i] = g\n",
    "\n",
    "with torch.no_grad():\n",
    "    Gs = F.normalize(Gs, p=2, dim=1)\n",
    "    C = torch.matmul(Gs, Gs.T)\n",
    "del Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(C.detach().cpu().numpy(), cmap='CMRmap', vmin=-1, vmax=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's try random projections of varying dimension and sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseProjectionOperator:\n",
    "    \"\"\"\n",
    "    Note: I think the sparsity is off by a factor of two here.\n",
    "    \"\"\"\n",
    "    def __init__(self, original_dim, projection_dim, sparsity, seed=0, device='cpu'):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed) if 'cuda' in device else None\n",
    "        self.device = torch.device(device)\n",
    "        self.original_dim = original_dim\n",
    "        self.lambda_ = original_dim * (1 - sparsity)\n",
    "        num_entries = torch.poisson(self.lambda_ * torch.ones(projection_dim, device=device)).int()\n",
    "        max_entries = num_entries.max()\n",
    "        self.positives = torch.randint(0, original_dim, (projection_dim, max_entries), device=device)\n",
    "        self.negatives = torch.randint(0, original_dim, (projection_dim, max_entries), device=device)\n",
    "        masks = torch.arange(max_entries, device=device).expand(projection_dim, max_entries) < num_entries.unsqueeze(-1)\n",
    "        self.positives = self.positives * masks\n",
    "        self.negatives = self.negatives * masks\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        assert x.device == self.device, \"device mismatch between projection and input\"\n",
    "        assert x.shape[-1] == self.original_dim, \"input dimension mismatch\"\n",
    "        y = x[self.positives].sum(-1) - x[self.negatives].sum(-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n",
    "sparsities = [0.999999, 0.99999, 0.9999, 0.999, 0.99, 0.9]\n",
    "\n",
    "Cs = dict()\n",
    "\n",
    "# d_proj = 1000\n",
    "# sparsity = 0.999\n",
    "for sparsity in tqdm(sparsities):\n",
    "    for d_proj in ds:\n",
    "        R = SparseProjectionOperator(len_g, d_proj, sparsity, seed=0, device='cuda:0')\n",
    "        Gs = torch.zeros((S, d_proj), device=device)\n",
    "        for i, idx in enumerate(subset_token_idxs):\n",
    "            model.zero_grad()\n",
    "            document, l = get_context(idx)\n",
    "            prompt = document['text']\n",
    "            tokens = tokenizer(prompt, return_tensors='pt', max_length=1024, truncation=True).to(device)\n",
    "            logits = model(**tokens).logits\n",
    "            targets = tokens.input_ids\n",
    "            ls = torch.nn.functional.cross_entropy(logits[0, :-1, :], targets[0, 1:], reduction='none')\n",
    "            ls_l = ls[l-1]\n",
    "            ls_l.backward()\n",
    "            g = get_flattened_gradient(model, highsignal_names)\n",
    "            with torch.no_grad():\n",
    "                Gs[i] = R(g)\n",
    "        with torch.no_grad():\n",
    "            Gs = F.normalize(Gs, p=2, dim=1)\n",
    "            C = torch.matmul(Gs, Gs.T)\n",
    "        Cs[(d_proj, sparsity)] = C.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if there's a memory leak somewhere\n",
    "# let's just plot the results we got before the memory error\n",
    "# 6 x 8\n",
    "plt.figure(figsize=(15, 12))\n",
    "i = 0\n",
    "for sparsity in sparsities:\n",
    "    for d_proj in ds:\n",
    "        plt.subplot(6, 9, i+1)\n",
    "        C = Cs[(d_proj, sparsity)]\n",
    "        plt.imshow(C, cmap='CMRmap', vmin=-1, vmax=1)\n",
    "        plt.title(f\"d_proj={d_proj}, sparsity={sparsity}\", fontsize=6)\n",
    "        # remove all ticks and labels from x and y axes\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idxs, C = torch.load(\"../results/paper-replication/pythia-70m-v0_143000_0.14426950408889636_50_10000_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 126, 60\n",
    "# 71 is number sequence continuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_idxs = indices[labels_sorted_by_freq[70]] + indices[labels_sorted_by_freq[59]]\n",
    "C_part = C[sim_idxs, :][:, sim_idxs]\n",
    "plt.imshow(C_part, cmap=\"CMRmap\", vmin=-0.2, vmax=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_token_idxs = [token_idxs[i] for i in sim_idxs]\n",
    "S = len(subset_token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n",
    "sparsities = [0.999999, 0.99999, 0.9999, 0.999, 0.99, 0.9]\n",
    "\n",
    "Cs = dict()\n",
    "\n",
    "for sparsity in tqdm(sparsities):\n",
    "    for d_proj in ds:\n",
    "        R = SparseProjectionOperator(len_g, d_proj, sparsity, seed=0, device='cuda:0')\n",
    "        Gs = torch.zeros((S, d_proj), device=device)\n",
    "        for i, idx in enumerate(subset_token_idxs):\n",
    "            model.zero_grad()\n",
    "            document, l = get_context(idx)\n",
    "            prompt = document['text']\n",
    "            tokens = tokenizer(prompt, return_tensors='pt', max_length=1024, truncation=True).to(device)\n",
    "            logits = model(**tokens).logits\n",
    "            targets = tokens.input_ids\n",
    "            ls = torch.nn.functional.cross_entropy(logits[0, :-1, :], targets[0, 1:], reduction='none')\n",
    "            ls_l = ls[l-1]\n",
    "            ls_l.backward()\n",
    "            g = get_flattened_gradient(model, highsignal_names)\n",
    "            with torch.no_grad():\n",
    "                Gs[i] = R(g)\n",
    "        with torch.no_grad():\n",
    "            Gs = F.normalize(Gs, p=2, dim=1)\n",
    "            C = torch.matmul(Gs, Gs.T)\n",
    "        Cs[(d_proj, sparsity)] = C.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if there's a memory leak somewhere\n",
    "# let's just plot the results we got before the memory error\n",
    "# 6 x 8\n",
    "plt.figure(figsize=(15, 12))\n",
    "i = 0\n",
    "for sparsity in sparsities:\n",
    "    for d_proj in ds:\n",
    "        plt.subplot(6, 9, i+1)\n",
    "        C = Cs[(d_proj, sparsity)]\n",
    "        plt.imshow(C, cmap='CMRmap', vmin=-0.2, vmax=0.2)\n",
    "        plt.title(f\"d_proj={d_proj}, sparsity={sparsity}\", fontsize=6)\n",
    "        # remove all ticks and labels from x and y axes\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idxs, C = torch.load(\"../results/paper-replication/pythia-70m-v0_143000_0.14426950408889636_50_10000_v1.pt\")\n",
    "C_part = C[sim_idxs, :][:, sim_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(C_part, cmap=\"CMRmap\", vmin=-0.2, vmax=0.2)\n",
    "plt.title(\"full gradient\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "d_proj = 2000\n",
    "sparsity = 0.999\n",
    "C_sparse = Cs[(d_proj, sparsity)]\n",
    "plt.imshow(C_sparse, cmap=\"CMRmap\", vmin=-0.2, vmax=0.2)\n",
    "plt.title(f\"d_proj={d_proj}, sparsity={sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5854332"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae1145e37e4470281b5713beb27cae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idxs = list(range(10000, 20000, 100))\n",
    "histograms = []\n",
    "for idx in tqdm(idxs):\n",
    "    model.zero_grad()\n",
    "    document, l = get_context(idx)\n",
    "    prompt = document['text']\n",
    "    tokens = tokenizer(prompt, return_tensors='pt', max_length=1024, truncation=True).to(device)\n",
    "    logits = model(**tokens).logits\n",
    "    targets = tokens.input_ids\n",
    "    ls = torch.nn.functional.cross_entropy(logits[0, :-1, :], targets[0, 1:], reduction='none')\n",
    "    ls_l = ls[l-1]\n",
    "    ls_l.backward()\n",
    "    g = get_flattened_gradient(model, highsignal_names)\n",
    "    g_np = g.detach().cpu().numpy()\n",
    "    hist, bin_edges = np.histogram(g_np, bins=10000)\n",
    "    centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    histograms.append((centers, hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "    centers, hist = histograms[i]\n",
    "    plt.plot(centers, hist, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"gradient component value\")\n",
    "    plt.ylabel(\"frequency\")\n",
    "plt.xlim(-10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_projection_matrix_sloppy(original_dim, projection_dim, sparsity):\n",
    "    \"\"\"\n",
    "    For speed, uses poisson distribution to approximate binomial distribution \n",
    "    and also doesn't check for collisions in the indices -- samples indices\n",
    "    with replacement.\n",
    "\n",
    "    Args:\n",
    "        original_dim (int): input dimension of projection\n",
    "        projection_dim (int): output dimension of projection\n",
    "        sparsity (float): probability that an entry is zero\n",
    "    \"\"\"\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    values = []\n",
    "    lambda_ = original_dim * (1 - sparsity)\n",
    "    for i in range(projection_dim):\n",
    "        rowi_num_entries = int(torch.poisson(torch.tensor([lambda_])).item())\n",
    "        rowi_col_indices = torch.randint(0, original_dim-1, (rowi_num_entries,))\n",
    "        rowi_values = 2*torch.randint(0, 2, (rowi_num_entries,)) - 1\n",
    "        row_indices.append(torch.ones((rowi_num_entries,)) * i)\n",
    "        col_indices.append(rowi_col_indices)\n",
    "        values.append(rowi_values)\n",
    "    row_indices = torch.cat(row_indices)\n",
    "    col_indices = torch.cat(col_indices)\n",
    "    indices = torch.stack([row_indices, col_indices])\n",
    "    values = torch.cat(values)\n",
    "    return torch.sparse_coo_tensor(indices, values, (projection_dim, original_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_projection_operator(original_dim, projection_dim, sparsity, seed=0):\n",
    "    \"\"\"\n",
    "    Closure that returns a function that computes the projection. Instead\n",
    "    of using a sparse matrix implementation, this simply stores a list\n",
    "    of +1 indices and -1 indices for each row.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    lambda_  = original_dim * (1 - sparsity)\n",
    "    num_entries = torch.poisson(lambda_ * torch.ones(projection_dim)).int()\n",
    "    max_entries = num_entries.max()\n",
    "    positives = torch.randint(0, original_dim, (projection_dim, max_entries))\n",
    "    negatives = torch.randint(0, original_dim, (projection_dim, max_entries))\n",
    "    masks = torch.arange(max_entries).expand(projection_dim, max_entries) < num_entries.unsqueeze(-1)\n",
    "    positives = positives * masks\n",
    "    negatives = negatives * masks\n",
    "    def project(x):\n",
    "        y = x[positives].sum(-1) - x[negatives].sum(-1)\n",
    "        return y\n",
    "    return project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's not worry too much about efficiency right now\n",
    "# we want to perform a sparse random projection, so the entries are just +1, (0), and -1\n",
    "\n",
    "def get_indices(n_total, n_subset):\n",
    "    indices_set = set()\n",
    "    while len(indices_set) < n_subset:\n",
    "        indices_set.add(torch.randint(0, n_total, (1,)).item())\n",
    "    subset_indices = torch.tensor(list(indices_set))\n",
    "    return subset_indices\n",
    "\n",
    "def sparse_projection_matrix(original_dim, projection_dim, sparsity):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        original_dim (int): input dimension of projection\n",
    "        projection_dim (int): output dimension of projection\n",
    "        sparsity (float): probability that an entry is zero\n",
    "    \"\"\"\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    values = []\n",
    "    for i in range(projection_dim):\n",
    "        rowi_num_entries = int(torch.distributions.Binomial(original_dim, 1-sparsity).sample().item())\n",
    "        rowi_col_indices = get_indices(original_dim, rowi_num_entries)\n",
    "        rowi_values = 2*torch.randint(0, 2, (rowi_num_entries,)) - 1\n",
    "        row_indices.append(torch.ones((rowi_num_entries,)) * i)\n",
    "        col_indices.append(rowi_col_indices)\n",
    "        values.append(rowi_values)\n",
    "    row_indices = torch.cat(row_indices)\n",
    "    col_indices = torch.cat(col_indices)\n",
    "    indices = torch.stack([row_indices, col_indices])\n",
    "    values = torch.cat(values)\n",
    "    return torch.sparse_coo_tensor(indices, values, (projection_dim, original_dim))\n",
    "\n",
    "def sparse_projection_matrix_sloppy(original_dim, projection_dim, sparsity):\n",
    "    \"\"\"\n",
    "    For speed, uses poisson distribution to approximate binomial distribution \n",
    "    and also doesn't check for collisions in the indices -- samples indices\n",
    "    with replacement.\n",
    "\n",
    "    Args:\n",
    "        original_dim (int): input dimension of projection\n",
    "        projection_dim (int): output dimension of projection\n",
    "        sparsity (float): probability that an entry is zero\n",
    "    \"\"\"\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    values = []\n",
    "    lambda_ = original_dim * (1 - sparsity)\n",
    "    for i in range(projection_dim):\n",
    "        rowi_num_entries = int(torch.poisson(torch.tensor([lambda_])).item())\n",
    "        rowi_col_indices = torch.randint(0, original_dim-1, (rowi_num_entries,))\n",
    "        rowi_values = 2*torch.randint(0, 2, (rowi_num_entries,)) - 1\n",
    "        row_indices.append(torch.ones((rowi_num_entries,)) * i)\n",
    "        col_indices.append(rowi_col_indices)\n",
    "        values.append(rowi_values)\n",
    "    row_indices = torch.cat(row_indices)\n",
    "    col_indices = torch.cat(col_indices)\n",
    "    indices = torch.stack([row_indices, col_indices])\n",
    "    values = torch.cat(values)\n",
    "    return torch.sparse_coo_tensor(indices, values, (projection_dim, original_dim))\n",
    "\n",
    "def sparse_projection_matrix_very_sloppy(original_dim, projection_dim, sparsity):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        original_dim (int): input dimension of projection\n",
    "        projection_dim (int): output dimension of projection\n",
    "        sparsity (float): probability that an entry is zero\n",
    "    \"\"\"\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    values = []\n",
    "    for i in range(projection_dim):\n",
    "        rowi_num_entries = int((1-sparsity) * original_dim)\n",
    "        rowi_col_indices = torch.randint(0, original_dim, (rowi_num_entries,))\n",
    "        rowi_values = 2*torch.randint(0, 2, (rowi_num_entries,)) - 1\n",
    "        row_indices.append(torch.ones((rowi_num_entries,)) * i)\n",
    "        col_indices.append(rowi_col_indices)\n",
    "        values.append(rowi_values)\n",
    "    row_indices = torch.cat(row_indices)\n",
    "    col_indices = torch.cat(col_indices)\n",
    "    indices = torch.stack([row_indices, col_indices])\n",
    "    values = torch.cat(values)\n",
    "    return torch.sparse_coo_tensor(indices, values, (projection_dim, original_dim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phase-changes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
